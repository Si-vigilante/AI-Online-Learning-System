{
  "qaQuestions": [
    {
      "id": "frtb1vf3",
      "title": "我好饿",
      "content": "今天吃点啥比较好？",
      "author": "User-3761",
      "createdAt": 1765792676911,
      "answers": [
        {
          "id": "3b35ldns",
          "content": "我吃了凉拌菜",
          "author": "User-3761",
          "createdAt": 1765797759972
        },
        {
          "id": "i034nhm2",
          "content": "不错",
          "author": "User-3761",
          "createdAt": 1765797797157
        }
      ],
      "acceptedAnswerId": "i034nhm2"
    },
    {
      "id": "seed-qa-1",
      "title": "如何理解反向传播中的梯度消失？",
      "content": "在多层网络中梯度会逐层衰减，有什么实用的规避策略？",
      "author": "Alice",
      "createdAt": 1734200000000,
      "answers": [
        {
          "id": "seed-ans-1",
          "content": "可以使用ReLU/LeakyReLU、批归一化、残差结构，并合理初始化权重。",
          "author": "Bob",
          "createdAt": 1734203600000
        }
      ],
      "acceptedAnswerId": "seed-ans-1"
    },
    {
      "id": "seed-qa-2",
      "title": "文本分类任务中，BERT 还是 RNN 更合适？",
      "content": "课程项目需要做新闻分类，是否必须用 BERT？",
      "author": "Carol",
      "createdAt": 1734210000000,
      "answers": [],
      "acceptedAnswerId": null
    }
  ],
  "forumPosts": [
    {
      "id": "8owdem2i",
      "title": "张大佑今天吃了什么？",
      "content": "没有人知道",
      "author": "User-3761",
      "createdAt": 1765798008125,
      "comments": []
    },
    {
      "id": "seed-post-1",
      "title": "分享：我用残差网络提升了 5% 的准确率",
      "content": "在课程作业里把基础 CNN 换成 ResNet18，配合余弦退火学习率，准确率有 5% 提升。",
      "author": "Dave",
      "createdAt": 1734206400000,
      "comments": [
        {
          "id": "seed-comment-1",
          "content": "有代码仓库吗？想参考一下实现细节。",
          "author": "Eve",
          "createdAt": 1734208200000
        }
      ]
    },
    {
      "id": "seed-post-2",
      "title": "期中报告写作提纲",
      "content": "我整理了“背景-方法-实验-结论”的提纲，附带常见坑点，欢迎补充。",
      "author": "Frank",
      "createdAt": 1734213600000,
      "comments": []
    }
  ],
  "examPapers": [
    {
      "id": "g3olcrgk",
      "title": "AI 测验",
      "courseId": "deep-learning",
      "createdBy": "Sin",
      "createdAt": "2025-12-16T08:00:44.658Z",
      "difficulty": "中等",
      "knowledgeScope": [],
      "durationMinutes": 20,
      "totalScore": 49,
      "questions": [
        {
          "id": "32d07088-4244-4749-aa74-0f92315dc387",
          "type": "single",
          "stem": "在深度学习中，使用 ReLU 激活函数的主要优点是？",
          "options": [
            {
              "key": "A",
              "text": "输出恒为正，避免数值不稳定"
            },
            {
              "key": "B",
              "text": "简化计算并缓解梯度消失"
            },
            {
              "key": "C",
              "text": "保证输出均值为 0"
            },
            {
              "key": "D",
              "text": "天然抵抗过拟合"
            }
          ],
          "answer": [
            "B"
          ],
          "explanation": "ReLU 计算简单且在正区间梯度恒为 1，可缓解梯度消失问题。",
          "knowledgePoints": [
            "激活函数",
            "梯度消失"
          ],
          "difficulty": "中等",
          "score": 5
        },
        {
          "id": "31b256f1-f01d-4e0c-94d5-10d03cf34b04",
          "type": "multiple",
          "stem": "以下哪些操作可以缓解梯度爆炸？",
          "options": [
            {
              "key": "A",
              "text": "梯度裁剪"
            },
            {
              "key": "B",
              "text": "使用较小的学习率"
            },
            {
              "key": "C",
              "text": "更换为 Sigmoid 激活函数"
            },
            {
              "key": "D",
              "text": "使用残差连接"
            }
          ],
          "answer": [
            "A",
            "B",
            "D"
          ],
          "explanation": "常用手段包括梯度裁剪、降低学习率、引入残差/规范化，Sigmoid 可能加重梯度消失。",
          "knowledgePoints": [
            "梯度爆炸",
            "优化"
          ],
          "difficulty": "中等",
          "score": 6
        },
        {
          "id": "3e76c21a-e9b0-46ff-b42f-3e0a2396571e",
          "type": "short",
          "stem": "简述反向传播算法的核心步骤。",
          "referenceAnswer": "前向计算得到损失，逐层按链式法则计算梯度，按参数维度累计梯度并应用优化器更新权重，可结合正则与梯度裁剪。",
          "gradingRubric": [
            {
              "item": "说明前向-损失计算",
              "points": 2
            },
            {
              "item": "链式法则逐层回传",
              "points": 3
            },
            {
              "item": "参数更新与优化器/正则",
              "points": 3
            }
          ],
          "knowledgePoints": [
            "反向传播",
            "优化"
          ],
          "difficulty": "中等",
          "score": 8
        },
        {
          "id": "jfbdrp6h",
          "type": "single",
          "stem": "在训练深层神经网络时，为缓解梯度消失问题，下列哪种方法通常不被采用？",
          "knowledgePoints": [
            "梯度消失与梯度爆炸的成因与解决策略"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "梯度消失问题常见于深层网络和某些激活函数的反向传播过程中。Sigmoid 激活函数的导数最大值仅为0.25，且容易饱和（输出接近0或1时导数趋近于0），链式法则中多层叠加会使得梯度急剧减小，因此它是导致梯度消失的一个常见原因，而不是解决方案。ReLU 激活函数在正区间的导数恒为1，有效缓解了梯度消失；Batch Normalization 通过规范化中间层激活值的分布，使得梯度更稳定；残差连接通过恒等映射的短路路径，允许梯度直接反向传播，都能有效缓解梯度消失。",
          "options": [
            {
              "key": "A",
              "text": "使用 Sigmoid 激活函数"
            },
            {
              "key": "B",
              "text": "使用 ReLU 激活函数"
            },
            {
              "key": "C",
              "text": "使用 Batch Normalization"
            },
            {
              "key": "D",
              "text": "使用残差连接（ResNet）"
            }
          ],
          "answer": [
            "A"
          ]
        },
        {
          "id": "pcpqwvk1",
          "type": "single",
          "stem": "在卷积神经网络中，对于一个输入尺寸为 32×32×3 的图像，使用 10 个 5×5 的卷积核进行卷积，步长为1，填充为0，则输出特征图的尺寸是多少？",
          "knowledgePoints": [
            "卷积神经网络结构"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "输出特征图的空间尺寸计算公式为：输出尺寸 = (输入尺寸 - 卷积核尺寸 + 2×填充) / 步长 + 1。本题输入尺寸为32，卷积核尺寸为5，填充为0，步长为1，代入公式得 (32 - 5 + 0) / 1 + 1 = 28。因此空间尺寸为 28×28。输出特征图的深度（通道数）等于卷积核的个数，即10。所以最终输出特征图尺寸为 28×28×10。",
          "options": [
            {
              "key": "A",
              "text": "28×28×10"
            },
            {
              "key": "B",
              "text": "32×32×10"
            },
            {
              "key": "C",
              "text": "28×28×3"
            },
            {
              "key": "D",
              "text": "32×32×3"
            }
          ],
          "answer": [
            "A"
          ]
        },
        {
          "id": "7jkydzgm",
          "type": "multiple",
          "stem": "关于循环神经网络（RNN）及其变体，以下哪些说法是正确的？",
          "knowledgePoints": [
            "循环神经网络与序列建模"
          ],
          "difficulty": "中等",
          "score": 10,
          "explanation": "A正确，标准RNN反向传播通过时间（BPTT）涉及矩阵的连乘，可能导致梯度指数级减小（消失）或增大（爆炸）。B正确，LSTM的门控机制（特别是遗忘门）允许信息有选择地通过，有效缓解了梯度消失，增强了长距离记忆能力。C正确，GRU是LSTM的一种简化变体，将输入门和遗忘门合并为更新门，减少了参数，通常性能接近LSTM。D正确，梯度截断通过设定阈值直接限制梯度的大小，可以防止梯度爆炸，但对梯度消失没有帮助。E错误，RNN架构灵活，可以通过编码器-解码器等结构处理输入输出序列长度不同的任务（如机器翻译）。",
          "options": [
            {
              "key": "A",
              "text": "标准RNN在训练时可能存在梯度消失或梯度爆炸问题，尤其在处理长序列时。"
            },
            {
              "key": "B",
              "text": "LSTM通过引入输入门、遗忘门和输出门，比标准RNN能更好地捕捉长距离依赖关系。"
            },
            {
              "key": "C",
              "text": "GRU将LSTM的输入门和遗忘门合并为更新门，结构更简单，但在许多任务上与LSTM性能相当。"
            },
            {
              "key": "D",
              "text": "处理长序列时，梯度截断是解决梯度爆炸的一种常用策略，但它无法缓解梯度消失问题。"
            },
            {
              "key": "E",
              "text": "RNN及其变体（LSTM, GRU）只能处理等长的输入序列和输出序列。"
            }
          ],
          "answer": [
            "A",
            "B",
            "C",
            "D"
          ]
        },
        {
          "id": "9qigpi3z",
          "type": "tf",
          "stem": "感知机是单层神经网络，它只能处理线性可分问题。",
          "knowledgePoints": [
            "感知机"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "该陈述正确。感知机是最简单的前馈神经网络，仅由输入层和单个输出层（无隐藏层）构成。其本质是一个线性分类器，使用阶跃函数作为激活函数。根据 Rosenblatt 的感知机收敛定理，只有当数据集是线性可分时，感知机算法才能保证收敛到一个解。对于非线性可分问题（如异或问题），单层感知机无法解决。",
          "answer": true
        },
        {
          "id": "568wpiyz",
          "type": "tf",
          "stem": "Dropout 是一种在训练时通过随机丢弃部分神经元来防止过拟合的正则化技术，在测试阶段所有神经元都参与前向传播，但每个神经元的输出需要乘以训练时被保留的概率 p（即 dropout rate）。",
          "knowledgePoints": [
            "正则化方法"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "该陈述的后半部分错误。Dropout 在测试阶段，所有神经元确实都参与前向传播（即不再随机丢弃）。但是，为了保持训练和测试时网络输出的期望值一致，常见的实现方式是在训练时对保留下来的神经元的输出进行缩放（乘以 1/(1-p)），称为“inverted dropout”。而在测试时，神经元的输出不做任何缩放（即乘以1）。另一种等价但不常用的方法是在测试时对输出乘以保留概率 p。题干描述混淆了这两种缩放发生的阶段和对象。",
          "answer": true
        }
      ],
      "meta": {
        "model": "deepseek-ai/DeepSeek-V3.2",
        "tokens": 1900,
        "retries": 1,
        "source": "mixed"
      }
    },
    {
      "id": "7jv3bykr",
      "title": "计算机网络 测验",
      "courseId": "deep-learning",
      "createdBy": "Sin",
      "createdAt": "2025-12-16T05:33:29.245Z",
      "difficulty": "中等",
      "knowledgeScope": [
        "计算机网络"
      ],
      "durationMinutes": 20,
      "totalScore": 40,
      "questions": [
        {
          "id": "kzowhfvk",
          "type": "single",
          "stem": "在 TCP/IP 模型中，深度学习模型训练过程中，损失值等监控数据在应用层通常使用什么协议进行传输？",
          "knowledgePoints": [
            "TCP/IP 模型",
            "应用层协议"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "在深度学习训练监控场景中，通常使用 HTTP/HTTPS 协议来传输训练指标、模型参数等数据到可视化服务器（如 TensorBoard 后端）。FTP 用于文件传输，SMTP 用于邮件，DNS 用于域名解析，均不适用于实时监控数据流传输。",
          "options": [
            {
              "key": "A",
              "text": "FTP"
            },
            {
              "key": "B",
              "text": "SMTP"
            },
            {
              "key": "C",
              "text": "HTTP/HTTPS"
            },
            {
              "key": "D",
              "text": "DNS"
            }
          ],
          "answer": [
            "C"
          ]
        },
        {
          "id": "qhwuceik",
          "type": "single",
          "stem": "当使用分布式深度学习训练时，多个计算节点之间需要高速、低延迟地同步梯度参数。以下哪种网络技术最有助于优化这种同步通信的性能？",
          "knowledgePoints": [
            "计算机网络性能",
            "数据中心网络"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "RDMA（远程直接内存访问）允许一台计算机直接访问另一台计算机的内存，无需经过操作系统内核和 CPU 介入，从而显著降低延迟、提高吞吐量，非常适合分布式训练中频繁的梯度同步。VLAN 是虚拟局域网，用于逻辑分割；NAT 是地址转换；HTTP/2 是应用层协议，主要优化 Web 请求，均不是为高性能计算集群内部通信设计的核心技术。",
          "options": [
            {
              "key": "A",
              "text": "使用 VLAN 对训练流量进行隔离"
            },
            {
              "key": "B",
              "text": "在网络边界部署 NAT 网关"
            },
            {
              "key": "C",
              "text": "采用支持 RDMA 的网络（如 InfiniBand 或 RoCE）"
            },
            {
              "key": "D",
              "text": "将通信协议升级到 HTTP/2"
            }
          ],
          "answer": [
            "C"
          ]
        },
        {
          "id": "hinviwsp",
          "type": "single",
          "stem": "在部署一个基于卷积神经网络的图像分类服务时，客户端通过网络向服务器发送图片并进行推理。为了在保证一定分类准确率的同时减少网络传输延迟，以下哪种方法通常不可行？",
          "knowledgePoints": [
            "网络传输优化",
            "模型部署"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "在客户端进行完整的模型推理（选项 D）虽然能消除网络传输延迟，但通常不切实际，因为客户端（如手机、浏览器）的计算资源和模型支持能力有限，且违背了服务端部署、维护和更新模型的初衷。其他选项都是可行的优化方法：A 减少图片大小，B 使用模型压缩技术，C 使用内容分发网络加速图片传输。",
          "options": [
            {
              "key": "A",
              "text": "在客户端对图片进行有损压缩（如降低分辨率）后再上传"
            },
            {
              "key": "B",
              "text": "在服务端使用剪枝、量化等技术压缩模型，加快单次推理速度"
            },
            {
              "key": "C",
              "text": "使用 CDN 将服务部署在离用户更近的节点"
            },
            {
              "key": "D",
              "text": "将完整的深度学习模型直接部署在客户端，由客户端完成推理"
            }
          ],
          "answer": [
            "D"
          ]
        },
        {
          "id": "oy8lo4fq",
          "type": "multiple",
          "stem": "关于循环神经网络（RNN）训练中可能遇到的梯度爆炸问题及其与网络层的关联，以下哪些描述是正确的？",
          "knowledgePoints": [
            "循环神经网络",
            "梯度问题",
            "网络层"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "梯度爆炸是指训练过程中梯度值变得异常巨大，导致模型参数更新不稳定。它主要发生在深度网络或时间步很长的 RNN 中，因为反向传播时梯度会通过链式法则连续相乘（选项 A 正确）。爆炸的梯度在反向传播过程中会从输出层向输入层方向传递（选项 C 正确）。梯度爆炸是训练不稳定的问题，不是模型结构本身的固有组成部分（选项 B 错误）。梯度消失（梯度趋近于零）是 RNN 中长距离依赖问题的主要原因，而非梯度爆炸（选项 D 错误）。",
          "options": [
            {
              "key": "A",
              "text": "梯度爆炸的发生与网络层的深度或时间步的长度密切相关，因为梯度在多层/多时间步反向传播时可能连续相乘导致数值过大。"
            },
            {
              "key": "B",
              "text": "梯度爆炸是 RNN 隐藏层激活函数（如 tanh）的固有属性，无法避免。"
            },
            {
              "key": "C",
              "text": "发生梯度爆炸时，爆炸的梯度信号会沿网络从输出层向输入层方向传播。"
            },
            {
              "key": "D",
              "text": "梯度爆炸是导致 RNN 难以学习长距离依赖关系的根本原因。"
            }
          ],
          "answer": [
            "A",
            "C"
          ]
        },
        {
          "id": "gri9pk4d",
          "type": "multiple",
          "stem": "在构建一个用于实时视频分析的深度学习系统时，需要考虑网络传输因素。以下哪些做法有助于缓解网络带宽和延迟对系统性能的制约？",
          "knowledgePoints": [
            "实时系统",
            "网络传输",
            "模型优化"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "所有选项均为有效策略。A：在边缘设备进行初步处理（如目标检测），只将关键结果（如边界框坐标）而非原始视频流传回云端，大幅减少数据量。B：有损压缩视频流（如降低帧率、分辨率）是直接减少带宽占用的常用方法。C：客户端缓冲和预测可以平滑因网络波动带来的延迟，提升用户体验。D：更高效的网络协议（如基于 UDP 的 QUIC 或 RTP）相比传统 TCP 可能在实时流媒体场景下提供更低的延迟和更好的拥塞控制。",
          "options": [
            {
              "key": "A",
              "text": "采用边缘计算架构，在靠近数据源的设备上进行部分预处理或轻量级推理，仅将关键结果传输至中心服务器。"
            },
            {
              "key": "B",
              "text": "对输入的视频流进行有损压缩，在可接受的精度损失范围内降低码率。"
            },
            {
              "key": "C",
              "text": "在客户端实施自适应码率控制和缓冲区管理，以应对网络延迟波动。"
            },
            {
              "key": "D",
              "text": "使用专为实时流媒体设计的传输协议（如 RTP/RTCP），而不是标准的 HTTP/TCP。"
            }
          ],
          "answer": [
            "A",
            "B",
            "C",
            "D"
          ]
        },
        {
          "id": "rbhbsada",
          "type": "tf",
          "stem": "判断题：在典型的客户端-服务器深度学习推理服务中，使用 UDP 协议传输推理请求和结果比 TCP 协议更合适，因为 UDP 延迟更低，且推理服务对数据包的少量丢失不敏感。",
          "knowledgePoints": [
            "传输层协议",
            "深度学习服务"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "错误。虽然 UDP 延迟通常低于 TCP，但深度学习推理请求和结果是关键数据，必须保证完整、有序地送达。一个丢失的请求可能导致客户端得不到响应，一个丢失的结果（如分类标签或概率）会使整个服务失效。数据可靠性比低延迟更重要，因此通常使用 TCP（或基于 TCP 的 HTTP/gRPC）。",
          "answer": true
        },
        {
          "id": "0qvnmlvd",
          "type": "tf",
          "stem": "判断题：卷积神经网络（CNN）中，步幅（stride）大于 1 的卷积层会减少输出特征图的空间尺寸，这类似于在计算机网络中，数据包经过路由器时，IP 包头中的 TTL（生存时间）字段值减 1。两者都是一种旨在控制网络复杂或传播深度的“缩减”操作。",
          "knowledgePoints": [
            "卷积神经网络",
            "网络协议"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "错误。这个类比不恰当。CNN 中增大步幅是为了有意识地降低特征图分辨率（下采样），以减少计算量和参数，是模型结构设计的一部分。IP 包头中的 TTL 减 1 是为了防止数据包在网络中无限循环，当 TTL 减至 0 时数据包被丢弃，这是一种防止网络故障的机制，并非为了结构化地缩减网络规模。两者的目的和机制本质不同。",
          "answer": true
        },
        {
          "id": "ikf4wgcj",
          "type": "short",
          "stem": "请简述，在分布式深度学习训练中，参数服务器（Parameter Server）架构与 All-Reduce 同步架构（如 Ring-AllReduce）在工作机制和网络通信模式上的主要区别。",
          "knowledgePoints": [
            "分布式训练",
            "网络通信模式"
          ],
          "difficulty": "中等",
          "score": 5,
          "explanation": "参数服务器架构存在中心节点（服务器）与多个工作节点（Workers）。工作节点计算梯度并发送给参数服务器，服务器聚合梯度并更新参数，再将新参数分发给工作节点。通信模式是星形的“客户端-服务器”模式，存在中心瓶颈。All-Reduce 架构（如 Ring-AllReduce）中，所有计算节点地位对等，通过特定的环状或树状拓扑结构，在各节点间直接交换和聚合梯度数据，最终每个节点都得到完整的聚合结果。通信模式是去中心化的对等网络模式，通常能更好地利用网络带宽，避免单一瓶颈。",
          "referenceAnswer": "参数服务器架构采用中心化的星形拓扑，存在一个或多个中心服务器负责梯度的聚合与参数的分发，工作节点只与服务器通信，容易造成服务器带宽瓶颈。All-Reduce 架构（如 Ring-AllReduce）采用去中心化的对等拓扑（如环形），所有节点既接收也发送数据，通过多轮节点间直接通信合作完成梯度的全局聚合与同步，通信负载较均匀，能更好地利用集群的总带宽。",
          "gradingRubric": [
            {
              "criteria": "指出参数服务器架构的中心化（星形）通信特点",
              "score": 1.5
            },
            {
              "criteria": "指出 All-Reduce 架构的去中心化（对等/环形）通信特点",
              "score": 1.5
            },
            {
              "criteria": "对比两者在是否存在单一瓶颈/带宽利用上的差异",
              "score": 2
            }
          ]
        }
      ],
      "meta": {
        "model": "deepseek-ai/DeepSeek-V3.2",
        "tokens": 2451,
        "retries": 0,
        "source": "llm_only"
      }
    },
    {
      "id": "03jjsgbv",
      "title": "激活函数 测验",
      "courseId": "deep-learning",
      "createdBy": "Sin",
      "createdAt": "2025-12-16T05:23:18.815Z",
      "difficulty": "简单",
      "knowledgeScope": [
        "激活函数",
        "反向传播"
      ],
      "durationMinutes": 20,
      "totalScore": 35,
      "questions": [
        {
          "id": "zuvf05hm",
          "type": "single",
          "stem": "对于一个使用Sigmoid激活函数的神经元，在反向传播过程中，其输出的导数（即梯度）的最大可能值是多少？",
          "knowledgePoints": [
            "激活函数",
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "Sigmoid函数S(x)=1/(1+e^{-x})，其导数S'(x)=S(x)(1-S(x))。由于S(x)∈(0,1)，所以S'(x)的最大值在S(x)=0.5时取得，为0.5*(1-0.5)=0.25。在反向传播时，梯度会乘以此导数，因此神经元输出的局部梯度最大值为0.25。",
          "options": [
            {
              "key": "A",
              "text": "0.25"
            },
            {
              "key": "B",
              "text": "0.5"
            },
            {
              "key": "C",
              "text": "1.0"
            },
            {
              "key": "D",
              "text": "大于1"
            }
          ],
          "answer": [
            "A"
          ]
        },
        {
          "id": "7odq1a9n",
          "type": "single",
          "stem": "在深度神经网络中，使用ReLU激活函数相比于Sigmoid函数的一个关键优势是什么？",
          "knowledgePoints": [
            "激活函数",
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "ReLU函数为f(x)=max(0,x)。当输入为正时，其导数为常数1。在反向传播时，梯度可以无衰减地通过ReLU层，这有助于缓解梯度消失问题。而Sigmoid函数导数最大值仅为0.25，且在两端趋近于0，容易导致梯度消失。",
          "options": [
            {
              "key": "A",
              "text": "计算更复杂"
            },
            {
              "key": "B",
              "text": "总是输出正值"
            },
            {
              "key": "C",
              "text": "缓解梯度消失问题"
            },
            {
              "key": "D",
              "text": "输出范围在0到1之间"
            }
          ],
          "answer": [
            "C"
          ]
        },
        {
          "id": "ewmfq1wd",
          "type": "single",
          "stem": "在反向传播算法中，计算损失函数关于某一层权重参数的梯度时，首先需要计算的是什么？",
          "knowledgePoints": [
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "反向传播是链式法则的逐层应用。为了计算损失L对某一层权重W的梯度∂L/∂W，首先需要计算损失L对该层输出z的梯度∂L/∂z（即上游梯度），然后结合该层的输入a（前向传播的输出）来计算。具体地，对于线性层z=Wa+b，有∂L/∂W=(∂L/∂z)a^T。",
          "options": [
            {
              "key": "A",
              "text": "损失函数关于该层输入的梯度"
            },
            {
              "key": "B",
              "text": "损失函数关于该层输出的梯度"
            },
            {
              "key": "C",
              "text": "该层激活函数的导数"
            },
            {
              "key": "D",
              "text": "该层权重的初始值"
            }
          ],
          "answer": [
            "B"
          ]
        },
        {
          "id": "w4mqtsrl",
          "type": "multiple",
          "stem": "关于激活函数在反向传播中的作用，以下哪些说法是正确的？（注意：多选）",
          "knowledgePoints": [
            "激活函数",
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "激活函数引入了非线性，使得多层网络可以学习复杂函数。在反向传播中，需要计算激活函数对其输入的导数，这个局部梯度会与上游传来的梯度相乘，得到传递到前一层的新梯度。因此，激活函数的导数形状直接影响梯度流动，例如饱和区导数小可能导致梯度消失。激活函数本身通常没有需要直接优化的参数（其参数如Leaky ReLU的负斜率系数需要优化，但通常认为是网络超参数或固定值，一般不作为可学习参数通过反向传播更新，所以B说法需谨慎；标准ReLU没有可学习参数）。准确的说法是它影响梯度的计算和传播。",
          "options": [
            {
              "key": "A",
              "text": "激活函数引入了非线性，使神经网络可以逼近复杂函数。"
            },
            {
              "key": "B",
              "text": "激活函数通常没有需要训练的参数，但其导数是反向传播计算梯度时必须的部分。"
            },
            {
              "key": "C",
              "text": "在反向传播链式法则中，需要乘以激活函数对其输入的导数。"
            },
            {
              "key": "D",
              "text": "如果激活函数导数在某些区域为零（如ReLU在负输入区），则该区域神经元的梯度为零，权重无法更新。"
            }
          ],
          "answer": [
            "A",
            "C",
            "D"
          ]
        },
        {
          "id": "1rjndy9h",
          "type": "multiple",
          "stem": "以下哪些是可能导致神经网络训练中梯度消失的原因？（注意：多选）",
          "knowledgePoints": [
            "激活函数",
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "梯度消失是指在反向传播过程中，梯度逐层相乘变得极小，导致靠前的层几乎不更新。原因包括：1）使用具有饱和区的激活函数，如Sigmoid、Tanh，其导数在输入绝对值很大时接近于0，多层连乘后梯度指数级减小。2）深层网络本身，因为梯度需要经过多层连乘。3）权重初始化不当（如值过大）可能导致激活进入饱和区，但直接原因是激活函数导数小。优化器选择不当（如学习率过大）主要导致训练不稳定，而不是梯度消失的根本原因。",
          "options": [
            {
              "key": "A",
              "text": "使用了Sigmoid激活函数的深层网络"
            },
            {
              "key": "B",
              "text": "网络层数过深"
            },
            {
              "key": "C",
              "text": "使用了Adam优化器代替SGD"
            },
            {
              "key": "D",
              "text": "权重初始化过大导致激活值进入激活函数的饱和区"
            }
          ],
          "answer": [
            "A",
            "B",
            "D"
          ]
        },
        {
          "id": "bxf8g3dr",
          "type": "tf",
          "stem": "在反向传播中，计算梯度时总是从输出层向输入层逐层进行。",
          "knowledgePoints": [
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "正确。反向传播算法的核心是利用链式法则计算损失函数对网络中所有参数的梯度。这个过程从最后的输出层开始，计算损失对输出的梯度，然后逐层反向传播，计算损失对每一层权重和偏置的梯度。",
          "answer": true
        },
        {
          "id": "csjozvuf",
          "type": "tf",
          "stem": "ReLU激活函数在x>0时的导数为1，在x<0时的导数为0，因此在反向传播中，当输入为负时，梯度完全消失，该神经元之后的权重将永远无法更新。",
          "knowledgePoints": [
            "激活函数",
            "反向传播"
          ],
          "difficulty": "简单",
          "score": 5,
          "explanation": "错误。后半句“该神经元之后的权重将永远无法更新”不正确。对于某个使用ReLU的神经元，如果当前输入为负，其输出为0，导数为0，那么在这次反向传播中，来自该神经元的梯度为0，这意味着损失函数关于该神经元输入的梯度为0，因此该神经元自身的权重和偏置在这次更新中不会改变。但是，这并不意味着该神经元“之后”的权重无法更新。“之后”的权重接收来自多条路径的梯度，只要其他路径的梯度非零，它们仍然可以更新。此外，该神经元的输入在后续训练中可能变为正，从而恢复梯度流。ReLU的特性是可能造成神经元“死亡”（始终输出0），但并非该神经元之后的所有权重都永久冻结。",
          "answer": true
        }
      ],
      "meta": {
        "model": "deepseek-ai/DeepSeek-V3.2",
        "tokens": 1918,
        "retries": 0,
        "source": "llm_only"
      }
    }
  ],
  "examGrades": [],
  "reportAssignments": [
    {
      "id": "report-demo-1",
      "courseId": "deep-learning",
      "title": "深度学习课程报告",
      "description": "围绕反向传播与梯度问题写一篇综述，包含案例与改进方法。",
      "knowledgePoints": [
        "反向传播",
        "梯度消失",
        "梯度爆炸",
        "优化"
      ],
      "dueAt": null,
      "rubric": {
        "relevance": 0.25,
        "structure": 0.25,
        "coverage": 0.25,
        "language": 0.25,
        "originality": 0.1
      },
      "exemplar": {
        "title": "示例报告",
        "rawText": "本报告综述反向传播原理、梯度消失/爆炸成因及缓解方法，结合卷积网络与循环网络的实践案例，提出改进建议与实验对比。"
      },
      "createdBy": "teacher",
      "createdAt": "2025-12-16T05:36:36.164Z"
    }
  ],
  "reportSubmissions": [],
  "reportFeedback": []
}